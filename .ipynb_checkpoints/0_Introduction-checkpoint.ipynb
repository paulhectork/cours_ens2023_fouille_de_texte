{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1773ec58",
   "metadata": {},
   "source": [
    "# Introduction à la fouille de texte avec Python: le programme des festivités\n",
    "\n",
    "Il existe **beaucoup** de méthodes pour analyser du texte avec Python, en fonction des objectifs, de ce que l'on veut comprendre du texte.\n",
    "- La [stylométrie](https://fr.wikipedia.org/wiki/Stylom%C3%A9trie) permet d'étudier le style d'un.e auteur.ice avec une approche quantitative, statistique\n",
    "- Le [Traitement automatisé du langage](https://fr.wikipedia.org/wiki/Traitement_automatique_des_langues), ou TAL, regroupe tout un ensemble de méthodes pour analyser du texte. Il s'appuie beaucoup sur l'apprentissage machine.\n",
    "    - la reconnaissance d'entités nommées (personnes, lieux etc. mentionnés par un texte)\n",
    "    - le *topic modelling* (modélisation des thèmes dans un texte)\n",
    "    - *sentiment analysis* (analyse des sentiments dans un texte...)\n",
    "    - ...\n",
    "\n",
    "Notre approche est plus simple et ciblé: avec quelques éléments de base en Python, il s'agit de détecter des motifs récurrents dans un corpus pour l'analyser de manière statistique. \n",
    "\n",
    "Ce cours vise à introduire aux bases de la manipulation de texte et de l'analyse en Python. Il est composé de deux `notebooks`, avec chacun une étape différente:\n",
    "- **`1_fouille_texte`**: le notebook principal. Faire l'analyse et la visualisation d'un corpus en texte brut.\n",
    "- **`2_bonus_creation_corpus`**: un notebook bonus, qui montre comment le corpus utilisé dans le `notebook` 1 a été produit, en utilisant une `API`.\n",
    "\n",
    "Il ne s'agit **pas de tout comprendre**, mais plutôt de voir qu'avec des outils très simples, on peut créer et analyser des gros corpus. Si vous faites plus de python plus tard, les techniques vues ici rentrent très facilement : ) \n",
    "\n",
    "---\n",
    "\n",
    "## Question de recherche et données utilisées\n",
    "\n",
    "### Source des données\n",
    "\n",
    "À partir d'un corpus en texte brut (c'est-à-dire, sans éléments de mise en page), on va développer une petite chaîne de traitement d'analyse et de visualisation du corpus. Le corpus est issu du programme de recherche [Katabase](https://katabase.huma-num.fr/). Ce programme a constitué une base de donnée de catalogues de vente de manuscrits. Ces catalogues imprimés, datant du XIXe siècle au début du XXe siècle, sont OCRisés, encodés en TEI et enrichis par fouille de texte.\n",
    "\n",
    "Tous les manuscrits écrits par 20 auteur.ice.s du XVIIIe siècle mis en vente dans les catalogues de Katabase ont été extraits et organisés en 4 corpus. Les 20 auteur.ice.s sont classés en 4 genres littéraires avec 5 auteur.ice.s par corpus.\n",
    "\n",
    "### Question de recherche\n",
    "\n",
    "Quelle est la représentation de cinq genres littéraires du XVIIIe siècle (poésie, théâtre, roman, littérature d'idées) dans un corpus datant du XIXe siècle de catalogues de vente de manuscrits ?\n",
    "\n",
    "---\n",
    "\n",
    "## Chaîne de traitement globale\n",
    "\n",
    "Les notebooks sont dans l'ordre inverse de la chaîne de traitement: le notebook `1_fouille_texte` vient après la constitution du corpus, qui est décrite dans le notebook `2_bonus_creation_corpus`. Si vous avec le temps/courage, je conseille de faire le notebook 2: il introduit à l'utilisation des API, très très utiles en recherche pour créer des corpus à partir de sources en ligne (Wikidata, la BnF...).\n",
    "- **`2_bonus_creation_corpus`**: Constitution du corpus.\n",
    "    - les auteur.ice.s sont définis pour chaque genre\n",
    "    - des manuscrits de ces auteur.ice.s présents dans les catalogues Katabase sont récupérés sur le Web, avec l'API de Katabase. Cette étape est le point central du notebook.\n",
    "    - les données récupérés sont nettoyées et structurées obtenir 4 corpus en texte brut\n",
    "    - les corpus sont enregistrés dans des fichiers\n",
    "- **`1_fouille_texte`**:\n",
    "    - les fichiers enregistrés à l'étape précédente sont repris ici.\n",
    "    - le texte brut est structuré dans des `dictionnaires` Python: par détection de motifs, les différentes entrées sont séparées et les éléments importants de chaque entrée sont distingués\n",
    "    - les données à partir desquelles les graphiques seront construits sont extraites et ordonnées comme il faut\n",
    "    - des graphiques sont créées pour analyser le corpus\n",
    "\n",
    "---\n",
    "\n",
    "## Compétences vues\n",
    "\n",
    "### Compétences\n",
    "\n",
    "- **`1_fouille_texte`**: \n",
    "    - expressions régulières (aussi dites `regex`) pour détecter des motifs dans du texte\n",
    "    - manipulation des types de données basiques de Python: texte (`string`), nombres (entiers `int` et décimaux `float`), listes (`list`) et dictionnaires (`dict`)\n",
    "    - visualisation de données avec `Plotly`\n",
    "    - création de statistiques\n",
    "    - lecture de texte depuis des fichiers\n",
    "    - création de chemins avec la librairie `os`\n",
    "- **`2_bonus_creation_corpus`**:\n",
    "    - principes fondamentaux des API\n",
    "    - utilisation de `requests` pour faire des requêtes `HTTP` sur le Web avec Python\n",
    "    - détection de motifs et nettoyage de texte avec des regex\n",
    "    - écriture de fichiers texte\n",
    "    - création de chemins avec `os`\n",
    "\n",
    "### Librairies utilisées\n",
    "\n",
    "- `re` pour les regex\n",
    "- `plotly` pour les visualisations de données\n",
    "- `os` pour construire et lire des fichiers\n",
    "- `requests` (bonus) pour faire des requêtes sur une API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
