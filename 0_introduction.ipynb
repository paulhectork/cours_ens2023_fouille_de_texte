{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1773ec58",
   "metadata": {},
   "source": [
    "# Introduction à la fouille de texte avec Python: le programme des festivités\n",
    "\n",
    "Il existe **beaucoup** de méthodes pour analyser du texte avec Python, en fonction des objectifs, de ce que l'on veut comprendre du texte.\n",
    "- La [stylométrie](https://fr.wikipedia.org/wiki/Stylom%C3%A9trie) permet d'étudier le style d'un.e auteur.ice avec une approche quantitative, statistique\n",
    "- Le [Traitement automatisé du langage](https://fr.wikipedia.org/wiki/Traitement_automatique_des_langues), ou TAL, regroupe tout un ensemble de méthodes pour analyser du texte. Il s'appuie beaucoup sur l'apprentissage machine.\n",
    "    - la reconnaissance d'entités nommées (personnes, lieux etc. mentionnés par un texte)\n",
    "    - le *topic modelling* (modélisation des thèmes dans un texte)\n",
    "    - *sentiment analysis* (analyse des sentiments dans un texte...)\n",
    "    - ...\n",
    "\n",
    "Notre approche est plus simple et ciblé: avec quelques éléments de base en Python, il s'agit de détecter des motifs récurrents dans un corpus pour l'analyser de manière statistique. \n",
    "\n",
    "Ce cours vise à introduire aux bases de la manipulation de texte et de l'analyse en Python. Il est composé de deux `notebooks`, avec chacun une étape différente:\n",
    "- **`1_fouille_texte`**: le notebook principal. Faire l'analyse et la visualisation d'un corpus en texte brut.\n",
    "- **`2_bonus_creation_corpus`**: un notebook bonus, qui montre comment le corpus utilisé dans le `notebook` 1 a été produit, en utilisant une `API`.\n",
    "\n",
    "Il ne s'agit **pas de tout comprendre**, mais plutôt de voir qu'avec des outils très simples, on peut créer et analyser des gros corpus. Si vous faites plus de python plus tard, les techniques vues ici rentrent très facilement : ) \n",
    "\n",
    "---\n",
    "\n",
    "## Question de recherche et données utilisées\n",
    "\n",
    "### Source des données\n",
    "\n",
    "À partir d'un corpus en texte brut (c'est-à-dire, sans éléments de mise en page), on va développer une petite chaîne de traitement d'analyse et de visualisation du corpus. Le corpus est issu du programme de recherche [Katabase](https://katabase.huma-num.fr/). Ce programme a constitué une base de donnée de catalogues de vente de manuscrits. Ces catalogues imprimés, datant du XIXe siècle au début du XXe siècle, sont OCRisés, encodés en TEI et enrichis par fouille de texte.\n",
    "\n",
    "Tous les manuscrits écrits par 20 auteur.ice.s du XVIIIe siècle mis en vente dans les catalogues de Katabase ont été extraits et organisés en 4 corpus. Les 20 auteur.ice.s sont classés en 4 genres littéraires avec 5 auteur.ice.s par corpus.\n",
    "\n",
    "### Question de recherche\n",
    "\n",
    "**Quelle est la représentation de cinq genres littéraires du XVIIIe siècle (poésie, théâtre, roman, littérature d'idées) dans un corpus datant du XIXe siècle de catalogues de vente de manuscrits ?**\n",
    "\n",
    "---\n",
    "\n",
    "## Chaîne de traitement globale\n",
    "\n",
    "Les notebooks sont dans l'ordre inverse de la chaîne de traitement.\n",
    "- le notebook `2_bonus_creation_corpus` décrit la manière dont le corpus utilisé ici est créé\n",
    "- le notebook `1_fouille_texte` vient après la constitution du corpus et décrit l'analyse du corpus.\n",
    "\n",
    "Au travers des deux notebooks, il s'agit donc de constituer un corpus avec une API, le nettoyer et le transformer en texte brut, l'enregistrer, le rouvrir, le structurer et en extraire des informations, faire des statistiques et les analyser.\n",
    "\n",
    "Si vous avec le temps/courage, je conseille de faire le notebook 2: il introduit à l'utilisation des API, très très utiles en recherche pour créer des corpus à partir de sources en ligne (Wikidata, la BnF...).\n",
    "\n",
    "---\n",
    "\n",
    "## Compétences vues\n",
    "\n",
    "### Compétences\n",
    "\n",
    "- **`1_fouille_texte`**: \n",
    "    - expressions régulières (aussi dites `regex`) pour détecter des motifs dans du texte\n",
    "    - manipulation des types de données basiques de Python: texte (`string`), nombres (entiers `int` et décimaux `float`), listes (`list`) et dictionnaires (`dict`)\n",
    "    - visualisation de données avec `Plotly`\n",
    "    - création de statistiques\n",
    "    - lecture de texte depuis des fichiers\n",
    "    - création de chemins avec la librairie `os`\n",
    "- **`2_bonus_creation_corpus`**:\n",
    "    - principes fondamentaux des API\n",
    "    - utilisation de `requests` pour faire des requêtes `HTTP` sur le Web avec Python\n",
    "    - détection de motifs et nettoyage de texte avec des regex\n",
    "    - écriture de fichiers texte\n",
    "    - création de chemins avec `os`\n",
    "\n",
    "### Librairies utilisées\n",
    "\n",
    "- `re` pour les regex\n",
    "- `plotly` pour les visualisations de données\n",
    "- `os` pour construire et lire des fichiers\n",
    "- `requests` (bonus) pour faire des requêtes sur une API\n",
    "\n",
    "---\n",
    "\n",
    "## Ressources supplémentaires / pour aller plus loin\n",
    "\n",
    "- l'excellent [cours de Python](https://github.com/PonteIneptique/cours-python) du master *Technologies numériques appliquées à l'histoire*, donné à l'École des Chartes par Thibault Clérice pendant des années. Il s'addresse aux très grand.es débutant.es et finit par la création d'un site Web complet en Python, couvre toutes les bases et bien plus\n",
    "- le [*Programming historian*](https://programminghistorian.org/) et sa [version française](https://programminghistorian.org/fr/): ce site, fait par des chercheur.euse.s en humanités numériques, regroupe plein de tutoriels utiles pour en apprendre plus sur les humanités numériques. Il s'addresse à un public relativement débutant\n",
    "- Pour des publications en humanités numériques: \n",
    "    - le [*Journal of data mining and digital humanities*](https://jdmdh.episciences.org/page/natural-language-processing-for-digital-humanities), avec une approche très technique et orienté apprentissage machine. Il y a des articles en français et en anglais\n",
    "    - la revue [*Humanités numériques*](https://journals.openedition.org/revuehn/), en français\n",
    "    - pour les humanités numériques francophones, on trouve énormément d'articles en accès libre sur [HAL](https://hal.science/), il suffit de chercher le nom des chercheur.euse.s qui vous intéressent. Par exemple: https://hal.science/hal-03018381"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
